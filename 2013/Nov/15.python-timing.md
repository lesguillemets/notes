Python で時間を測るときに
---------------------------

python であるコードの実行時間を測るとき，ひとつの自然なやり方は

```python
#!/usr/bin/python
import time

start = time.time()
pass # time this!
end = time.time()

print end - start
```

これはなかなか直観的だ．ある部分を実行する前と実行した後直ちに時計を見て，その差を測る．
ただこれで時間を測定しようという時に，意外な（？）要因で差ができる事象に遭遇したので，その話．

`a = 3`, `b = 5` とそれぞれ代入し，`a` と `b` の大小比較をする，というのを
`for` の二重ループで合計 1000000 回やらせてみる．

さっきの最初の書き方では

```python
#!/usr/bin/python
import time

x = time.time()
for i in xrange(1000):
    for j in xrange(1000):
        a=3
        b=5
        if a < b:
            pass
        else:
            pass
y = time.time()
print y-x
```

結果はたとえば

```
0.125607967377
```

一方，最初にこれらの処理を関数にしておくと，

```python
x = time.time()
def g():
    for i in xrange(1000):
        for j in xrange(1000):
            a=3
            b=5
            if a < b:
                pass
            else:
                pass
y = time.time()
print y-x

x2 = time.time()
g()
y2 = time.time()
print y2-x2
```

こっちは

```
9.53674316406e-07
0.0537710189819
```

と，随分時間に差が出る(合計しても)．

このへんの理由がちょっとわからなくて，うーん，といった感じ．関数定義の時に最適化が働いたりするのかなぁ．

なお，今回の実行は vim の [`Quickrun`](https://github.com/thinca/vim-quickrun) で行いました．
一応それ以外の実行もやってみたけど，それによる差はなさそう（当然か）．

[今回使ったスクリプト](./Scripts/15Nov2013.timing.py)
